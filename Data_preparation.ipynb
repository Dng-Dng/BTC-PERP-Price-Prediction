{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81d0c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import dask.dataframe as dd \n",
    "\n",
    "\n",
    "#book_snapshot_5\n",
    "#liquidations\n",
    "#quotes\n",
    "#trades\n",
    "#derivative_ticker\n",
    "\n",
    "# path for all related files\n",
    "datasets = r\"C:\\Users\\danie\\Desktop\\master_project-env\\datasets\"\n",
    "exchanges = ['ftx_']#, 'binance-futures_','ftx_', 'bybit_']\n",
    "filetypes = ['book_snapshot_5','trades', 'quotes','derivative_ticker', 'liquidations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55df309",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\danie\\Desktop\\master_project-env\\datasets\"\n",
    "exchange = 'ftx'\n",
    "for filetype in tqdm(filetypes):\n",
    "    filename = glob.iglob(filetype+'*csv.gz')\n",
    "    files_path = glob.glob(path + '\\\\' + exchange + '_' + filetype + '_' + \"*.csv.gz\") #file path for all files\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for filename in files_path:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        files.append(df)\n",
    "    \n",
    "    #prepare combined file\n",
    "    combined = pd.concat(files, axis=0, ignore_index=True)\n",
    "    combined['timestamp'] = pd.to_datetime(combined['timestamp'], unit='us')\n",
    "    combined = combined.drop(['local_timestamp'], axis=1)\n",
    "    combined = combined.set_index('timestamp').sort_index()\n",
    "    combined = combined.dropna()\n",
    "    \n",
    "    combined.to_csv(path.replace('datasets', 'datasets_combined') + \"\\\\\" + exchange + \"_\"+ filetype \n",
    "                        + \".csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6164392d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 62889 entries, 2021-11-19 00:04:46.140152 to 2021-12-18 23:38:01.914502\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   exchange  62889 non-null  object \n",
      " 1   symbol    62889 non-null  object \n",
      " 2   id        62889 non-null  int64  \n",
      " 3   side      62889 non-null  object \n",
      " 4   price     62889 non-null  int64  \n",
      " 5   amount    62889 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "011582db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index        503112\n",
       "exchange    3773340\n",
       "symbol      4087785\n",
       "id           503112\n",
       "side        3813753\n",
       "price        503112\n",
       "amount       503112\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b476714",
   "metadata": {},
   "source": [
    "# Downsampling dataset to S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de5ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(exchange, file_type):\n",
    "    path = r\"C:\\Users\\danie\\Desktop\\master_project-env\\datasets_combined\"\n",
    "    # data[['exchange', 'symbol']] = data[['exchange', 'symbol']].apply(lambda x: x.astype('category'))\n",
    "    \n",
    "    #read file in chunks\n",
    "    chunksize = 10**6\n",
    "    combined_df = pd.DataFrame()\n",
    "    for chunk in tqdm(pd.read_csv(path + \"\\\\\" + exchange + \"_\" + filetype +\".csv.gz\", chunksize=chunksize)):\n",
    "\n",
    "        chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
    "        chunk = chunk.dropna()\n",
    "        chunk = chunk.set_index('timestamp')\n",
    "        chunk = chunk.resample('S').mean() #resample to S\n",
    "        combined_df = pd.concat([combined_df, chunk])\n",
    "\n",
    "    #find duplicate time indexes\n",
    "    duplicates = [index for index, value in enumerate(combined_df.index.duplicated(keep='first')) if value]\n",
    "\n",
    "    #replace second duplicate with mean for the duplicate pair\n",
    "    combined_df.reset_index(inplace=True)\n",
    "    for duplicate in tqdm(duplicates):\n",
    "        new_average = combined_df[duplicate-1: duplicate+1].mean()\n",
    "        combined_df = combined_df.replace(combined_df.iloc[duplicate], new_average)\n",
    "\n",
    "    duplicates_new = [duplicate-1 for duplicate in duplicates] #index for the other duplicate pairs\n",
    "    combined_df.drop(duplicates_new, inplace=True) #remove the duplicates\n",
    "    combined_df  = combined_df.set_index('timestamp')\n",
    "    \n",
    "    #forward fill data that's missing after downsampling\n",
    "    combined_df = combined_df.fillna(method='ffill')\n",
    "    combined_df.to_parquet(path + \"\\\\downsampled\\\\\" + exchange + \"_\" + filetype + \"_downsampled.parquet.gz\", compression='gzip')\n",
    "    \n",
    "    return print(\"done {}\".format(file_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b64b277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [01:54,  2.65s/it]\n",
      "  0%|                                                                                           | 0/40 [00:00<?, ?it/s]C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_2736\\249732487.py:22: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  new_average = combined_df[duplicate-1: duplicate+1].mean()\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [00:15<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done book_snapshot_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [00:23,  1.53s/it]\n",
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_2736\\249732487.py:22: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  new_average = combined_df[duplicate-1: duplicate+1].mean()\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done trades\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:52,  1.64s/it]\n",
      "  0%|                                                                                           | 0/27 [00:00<?, ?it/s]C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_2736\\249732487.py:22: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  new_average = combined_df[duplicate-1: duplicate+1].mean()\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [00:02<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done quotes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 333.04it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done derivative_ticker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.97it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done liquidations\n"
     ]
    }
   ],
   "source": [
    "exchange = 'ftx'\n",
    "for filetype in filetypes:\n",
    "    downsampling(exchange, filetype)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84140bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
